name: demo-eks-ci

on:
  workflow_dispatch: {}
  push:
    branches: ["main"]

# Only static values at top-level env
env:
  AWS_REGION: us-east-1
  ECR_REPO: demo-app
  NAMESPACE: default
  CLUSTER_NAME: demo-eks              # stable name now that we use remote state

permissions:
  id-token: write
  contents: read

jobs:

  terraform:
    name: Terraform Infra (S3 Remote State)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token:     ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      # 1) Compute and export TF variables for this job
      - name: Export TF variables
        run: |
          echo "TF_VAR_cluster_name=${{ env.CLUSTER_NAME }}" >> $GITHUB_ENV
          echo "TF_VAR_cluster_role_arn=${{ vars.CLUSTER_ROLE_ARN }}" >> $GITHUB_ENV
          echo "TF_VAR_node_role_arn=${{ vars.NODE_ROLE_ARN }}" >> $GITHUB_ENV

      # 2) Bootstrap S3 backend and DynamoDB lock if they don't exist
      - name: Bootstrap remote state (S3 bucket + DynamoDB lock)
        env:
          STATE_BUCKET: ${{ vars.TF_STATE_BUCKET }}
          LOCK_TABLE:   ${{ vars.TF_LOCK_TABLE }}
          REGION:       ${{ env.AWS_REGION }}
        run: |
          set -e
          if [ -z "$STATE_BUCKET" ] || [ -z "$LOCK_TABLE" ]; then
            echo "ERROR: Set repo Variables TF_STATE_BUCKET and TF_LOCK_TABLE first."; exit 1
          fi

          # Create S3 bucket if missing
          if ! aws s3api head-bucket --bucket "$STATE_BUCKET" 2>/dev/null; then
            echo "Creating S3 bucket: $STATE_BUCKET ..."
            aws s3api create-bucket --bucket "$STATE_BUCKET" --region "$REGION" --create-bucket-configuration LocationConstraint="$REGION"
            aws s3api put-bucket-versioning --bucket "$STATE_BUCKET" --versioning-configuration Status=Enabled
            aws s3api put-bucket-encryption --bucket "$STATE_BUCKET" --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'
          else
            echo "S3 bucket exists: $STATE_BUCKET"
          fi

          # Create DynamoDB lock table if missing
          if ! aws dynamodb describe-table --table-name "$LOCK_TABLE" >/dev/null 2>&1; then
            echo "Creating DynamoDB lock table: $LOCK_TABLE ..."
            aws dynamodb create-table \
              --table-name "$LOCK_TABLE" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --billing-mode PAY_PER_REQUEST
            aws dynamodb wait table-exists --table-name "$LOCK_TABLE"
          else
            echo "DynamoDB table exists: $LOCK_TABLE"
          fi

      # 3) Terraform init with backend-config (remote state)
      - name: Terraform init (remote state)
        working-directory: infra
        env:
          STATE_BUCKET: ${{ vars.TF_STATE_BUCKET }}
          LOCK_TABLE:   ${{ vars.TF_LOCK_TABLE }}
        run: |
          terraform init -upgrade \
            -backend-config="bucket=${STATE_BUCKET}" \
            -backend-config="key=terraform-eks-demo/${{ env.CLUSTER_NAME }}.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="dynamodb_table=${LOCK_TABLE}" \
            -backend-config="encrypt=true"

      # 4) If live resources exist but state is empty, import them to state (idempotent)
      - name: Import existing resources if needed
        working-directory: infra
        env:
          STATE_BUCKET: ${{ vars.TF_STATE_BUCKET }}
        run: |
          set -e

          # Check if the state object exists
          if aws s3api head-object --bucket "$STATE_BUCKET" --key "terraform-eks-demo/${{ env.CLUSTER_NAME }}.tfstate" >/dev/null 2>&1; then
            echo "Remote state exists; no import needed."
            exit 0
          fi

          echo "Remote state not found. Checking for existing live resources..."
          if aws eks describe-cluster --name "${{ env.CLUSTER_NAME }}" --region "${{ env.AWS_REGION }}" >/dev/null 2>&1; then
            echo "Cluster exists; importing into Terraform state..."
            terraform import aws_eks_cluster.this "${{ env.CLUSTER_NAME }}" || true

            # Import the default node group if it exists
            if aws eks list-nodegroups --cluster-name "${{ env.CLUSTER_NAME }}" --region "${{ env.AWS_REGION }}" --query 'nodegroups[]' --output text | grep -q '^demo$'; then
              terraform import aws_eks_node_group.default "${{ env.CLUSTER_NAME }}:demo" || true
            fi

            # Import ECR repo if exists
            if aws ecr describe-repositories --repository-names "${{ env.ECR_REPO }}" --region "${{ env.AWS_REGION }}" >/dev/null 2>&1; then
              terraform import aws_ecr_repository.demo "${{ env.ECR_REPO }}" || true
            fi
          else
            echo "Cluster does not exist; a fresh create will follow."
          fi

      # 5) Apply
      - name: Terraform apply
        working-directory: infra
        run: terraform apply -auto-approve


  build_and_push:
    name: Build & Push Image
    runs-on: ubuntu-latest
    needs: terraform
    outputs:
      image_uri: ${{ steps.push.outputs.image_uri }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token:     ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build & Push
        id: push
        env:
          REGISTRY:  ${{ steps.ecr.outputs.registry }}
          IMAGE_TAG: latest
        run: |
          set -e
          IMAGE_URI="$REGISTRY/${{ env.ECR_REPO }}:$IMAGE_TAG"
          docker build -t "$IMAGE_URI" ./app
          docker push "$IMAGE_URI"
          echo "image_uri=$IMAGE_URI" >> $GITHUB_OUTPUT


  deploy:
    name: Install Ingress & Deploy App
    runs-on: ubuntu-latest
    needs: build_and_push
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token:     ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: "v1.29.6"

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name "${{ env.CLUSTER_NAME }}" --region "${{ env.AWS_REGION }}"

      # Wait for control plane & nodes
      - name: Wait for EKS cluster ACTIVE
        run: aws eks wait cluster-active --name "${{ env.CLUSTER_NAME }}" --region "${{ env.AWS_REGION }}"

      - name: Wait for API /readyz
        run: |
          for i in {1..60}; do
            if kubectl get --raw='/readyz?verbose' >/dev/null 2>&1; then
              echo "API server fully ready"
              break
            fi
            echo "Waiting for API /readyz ($i/60)"
            sleep 5
          done

      - name: Wait for at least one Ready node
        run: |
          for i in {1..40}; do
            READY=$(kubectl get nodes --no-headers 2>/dev/null | awk '{print $2}' | grep -c '^Ready$' || true)
            if [ "$READY" -ge 1 ]; then
              echo "Node ready"
              kubectl get nodes
              break
            fi
            echo "Waiting for node Ready ($i/40)"
            sleep 10
          done

      # Install NGINX ingress (skip validation; retry)
      - name: Install ingress-nginx (retry)
        run: |
          set -e
          URL="https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/aws/deploy.yaml"
          echo "Installing ingress-nginx with retry..."
          for i in {1..5}; do
            if kubectl apply --validate=false -f "$URL"; then
              echo "Ingress applied OK"
              break
            fi
            echo "Retry $i/5 failed; sleeping 10s..."
            sleep 10
          done
          kubectl rollout status deployment/ingress-nginx-controller -n ingress-nginx --timeout=900s

      - name: Deploy app manifests
        env:
          IMAGE_URI: ${{ needs.build_and_push.outputs.image_uri }}
        run: |
          set -e
          sed "s|REPLACE_IMAGE|$IMAGE_URI|g" app/k8s/deployment.yaml \
            | kubectl apply --validate=false -n "${{ env.NAMESPACE }}" -f -
          kubectl apply --validate=false -n "${{ env.NAMESPACE }}" -f app/k8s/service.yaml
          kubectl apply --validate=false -n "${{ env.NAMESPACE }}" -f app/k8s/ingress.yaml

      - name: Wait for app rollout
        run: kubectl rollout status deployment/demo-app -n "${{ env.NAMESPACE }}" --timeout=600s

      - name: Show ingress & NGINX LB
        run: |
          kubectl get ingress -n "${{ env.NAMESPACE }}"
          kubectl get svc -n ingress-nginx