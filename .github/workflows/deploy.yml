name: demo-eks-ci

on:
  workflow_dispatch: {}
  push:
    branches: ["main"]

env:
  # Make sure this matches infra/main.tf region
  AWS_REGION: us-east-1
  EKS_CLUSTER: demo-eks
  ECR_REPO: demo-app
  NAMESPACE: default

permissions:
  id-token: write
  contents: read

jobs:

  terraform:
    name: Terraform Infra
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token:     ${{ secrets.AWS_SESSION_TOKEN }}  # leave blank if not using STS
          aws-region:            ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      # -------- NEW: Clean up a colliding cluster name from prior runs --------
      - name: Pre-clean existing EKS cluster (nodegroups + cluster)
        env:
          AWS_REGION:  ${{ env.AWS_REGION }}
          EKS_CLUSTER: ${{ env.EKS_CLUSTER }}
        run: |
          echo "Checking if EKS cluster ${EKS_CLUSTER} exists..."
          STATUS=$(aws eks describe-cluster --name "${EKS_CLUSTER}" --region "${AWS_REGION}" --query 'cluster.status' --output text 2>/dev/null || echo "NONE")
          if [ "$STATUS" != "NONE" ]; then
            echo "Cluster ${EKS_CLUSTER} exists with status: ${STATUS}"

            # If it's deleting already, just wait; if creating/updating, wait until ACTIVE then delete.
            if [ "$STATUS" != "ACTIVE" ]; then
              echo "Waiting for cluster to become ACTIVE or disappear..."
              for i in {1..60}; do
                STATUS=$(aws eks describe-cluster --name "${EKS_CLUSTER}" --region "${AWS_REGION}" --query 'cluster.status' --output text 2>/dev/null || echo "NONE")
                [ "$STATUS" = "ACTIVE" ] && break
                [ "$STATUS" = "NONE" ] && break
                echo "Current status: $STATUS (try $i/60)"
                sleep 20
              done
            fi

            # If still there, delete nodegroups then cluster
            STATUS=$(aws eks describe-cluster --name "${EKS_CLUSTER}" --region "${AWS_REGION}" --query 'cluster.status' --output text 2>/dev/null || echo "NONE")
            if [ "$STATUS" != "NONE" ]; then
              echo "Listing nodegroups..."
              NGS=$(aws eks list-nodegroups --cluster-name "${EKS_CLUSTER}" --region "${AWS_REGION}" --query 'nodegroups[]' --output text 2>/dev/null)
              if [ -n "$NGS" ]; then
                for NG in $NGS; do
                  echo "Deleting nodegroup: $NG"
                  aws eks delete-nodegroup --cluster-name "${EKS_CLUSTER}" --nodegroup-name "$NG" --region "${AWS_REGION}" || true
                done
                for NG in $NGS; do
                  echo "Waiting for nodegroup deletion: $NG"
                  for i in {1..90}; do
                    aws eks describe-nodegroup --cluster-name "${EKS_CLUSTER}" --nodegroup-name "$NG" --region "${AWS_REGION}" >/dev/null 2>&1
                    RC=$?
                    if [ $RC -ne 0 ]; then
                      echo "Nodegroup $NG deleted"
                      break
                    fi
                    echo "Still deleting nodegroup $NG (try $i/90)"
                    sleep 20
                  done
                done
              else
                echo "No nodegroups found."
              fi

              echo "Deleting cluster ${EKS_CLUSTER}..."
              aws eks delete-cluster --name "${EKS_CLUSTER}" --region "${AWS_REGION}" || true

              echo "Waiting for cluster deletion..."
              for i in {1..120}; do
                aws eks describe-cluster --name "${EKS_CLUSTER}" --region "${AWS_REGION}" >/dev/null 2>&1
                RC=$?
                if [ $RC -ne 0 ]; then
                  echo "Cluster ${EKS_CLUSTER} deleted."
                  break
                fi
                echo "Cluster still present (try $i/120)"
                sleep 20
              done
            fi
          else
            echo "No existing cluster named ${EKS_CLUSTER}."
          fi

      # -------- Existing pre-cleans to avoid ‘already exists’ on no-state runs --------
      - name: Pre-clean ECR repo & residual CW log group (safe/idempotent)
        env:
          AWS_REGION:  ${{ env.AWS_REGION }}
          ECR_REPO:    ${{ env.ECR_REPO }}
          EKS_CLUSTER: ${{ env.EKS_CLUSTER }}
        run: |
          echo "Checking ECR repo: $ECR_REPO"
          if aws ecr describe-repositories --repository-names "$ECR_REPO" --region "$AWS_REGION" >/dev/null 2>&1; then
            echo "Deleting existing ECR repo $ECR_REPO (force)..."
            aws ecr delete-repository --repository-name "$ECR_REPO" --force --region "$AWS_REGION" || true
          fi

          if aws logs describe-log-groups --log-group-name-prefix "/aws/eks/${EKS_CLUSTER}/cluster" --region "$AWS_REGION" \
             | grep -q "/aws/eks/${EKS_CLUSTER}/cluster"; then
            echo "Deleting old CloudWatch log group /aws/eks/${EKS_CLUSTER}/cluster ..."
            aws logs delete-log-group --log-group-name "/aws/eks/${EKS_CLUSTER}/cluster" --region "$AWS_REGION" || true
          fi

      - name: Terraform init
        working-directory: infra
        run: terraform init -upgrade

      - name: Terraform apply
        working-directory: infra
        run: terraform apply -auto-approve


  build_and_push:
    name: Build & Push Image
    runs-on: ubuntu-latest
    needs: terraform
    outputs:
      image_uri: ${{ steps.push.outputs.image_uri }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token:     ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build & Push
        id: push
        env:
          REGISTRY:  ${{ steps.ecr.outputs.registry }}
          IMAGE_TAG: latest   # for demo; prefer ${{ github.sha }} in real use
        run: |
          set -e
          IMAGE_URI="$REGISTRY/${{ env.ECR_REPO }}:$IMAGE_TAG"
          echo "Building image: $IMAGE_URI"
          docker build -t "$IMAGE_URI" ./app
          docker push "$IMAGE_URI"
          echo "image_uri=$IMAGE_URI" >> $GITHUB_OUTPUT


  deploy:
    name: Install Ingress & Deploy App
    runs-on: ubuntu-latest
    needs: build_and_push
    env:
      AWS_REGION:  ${{ env.AWS_REGION }}
      EKS_CLUSTER: ${{ env.EKS_CLUSTER }}
      NAMESPACE:   ${{ env.NAMESPACE }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token:     ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region:            ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: "v1.29.6"

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name "${EKS_CLUSTER}" --region "${AWS_REGION}"

      # Wait for EKS control-plane & nodes to be ready to avoid OpenAPI/cred errors
      - name: Wait for EKS cluster ACTIVE
        run: aws eks wait cluster-active --name "${EKS_CLUSTER}" --region "${AWS_REGION}"

      - name: Wait for API /readyz
        run: |
          for i in {1..60}; do
            if kubectl get --raw='/readyz?verbose' >/dev/null 2>&1; then
              echo "API server fully ready"
              break
            fi
            echo "Waiting for API /readyz ($i/60)"
            sleep 5
          done

      - name: Wait for at least one Ready node
        run: |
          for i in {1..40}; do
            READY=$(kubectl get nodes --no-headers 2>/dev/null | awk '{print $2}' | grep -c '^Ready$' || true)
            if [ "$READY" -ge 1 ]; then
              echo "Node ready"
              kubectl get nodes
              break
            fi
            echo "Waiting for node Ready ($i/40)"
            sleep 10
          done

      # Install ingress-nginx with retries and skip OpenAPI validation
      - name: Install ingress-nginx (retry)
        run: |
          set -e
          URL="https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/aws/deploy.yaml"

          echo "Installing ingress-nginx with retry..."
          for i in {1..5}; do
            if kubectl apply --validate=false -f "$URL"; then
              echo "Ingress applied OK"
              break
            fi
            echo "Retry $i/5 failed; sleeping 10s..."
            sleep 10
          done

          kubectl rollout status deployment/ingress-nginx-controller -n ingress-nginx --timeout=900s

      - name: Deploy app manifests
        env:
          IMAGE_URI: ${{ needs.build_and_push.outputs.image_uri }}
        run: |
          set -e
          sed "s|REPLACE_IMAGE|$IMAGE_URI|g" app/k8s/deployment.yaml \
            | kubectl apply --validate=false -n "${NAMESPACE}" -f -
          kubectl apply --validate=false -n "${NAMESPACE}" -f app/k8s/service.yaml
          kubectl apply --validate=false -n "${NAMESPACE}" -f app/k8s/ingress.yaml

      - name: Wait for app rollout
        run: kubectl rollout status deployment/demo-app -n "${NAMESPACE}" --timeout=600s

      - name: Show ingress & NGINX LB
        run: |
          kubectl get ingress -n "${NAMESPACE}"
          kubectl get svc -n ingress-nginx